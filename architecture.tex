%\pagebreak
\section{Architecture}\label{sec:arch}

We now  give  an overview of   \system architecture.  We  discuss  the
design goals that    motivate  a new architecture, which     separates
functionality  into three different  planes,  architecture components,
and the mechanisms to achieve the design goals.
\subsection{Design Goals}

{\bf  Scalable   policy management.}    Correctness conditions dictate that traffic must be correctly forwarded through a middlebox chain according to network policies.  Relying on routing
for traffic steering  is inefficient, because distributed
routing protocols are slow to converge during topology reconfiguration
and   provide only  coarse-grained   control  over  flows.   SDN-based
solutions  avoid this problem  by installing per-flow rules on network
switches, but these do not scale well since the SDN controller must be
involved  in per-flow decisions, and switches  have  limited memory to
store flow rules; in the worst case, each switch must install one rule
per TCP flow.

In \system,    we avoid both of    these scalability  impediments. The
protocol is designed such that the destination  address of each packet
identifies the next middlebox  or endpoint in  the path and the source
address identifies the sending  middlebox or endpoint. Doing so avoids
the need of routing tweaks  when network topology changes or endpoints
move.    Policy  management in \system  is   controlled by a logically
centralized controller,  but  packets  on the   data  plane are  never
redirected  to the policy server.   This is a  key difference from SDN
solutions  that queue packets  on the controller during flow migration
between middleboxes~\cite{OpenNF}.   Moreover, the  policy  server can
offload per-flow  decisions   to the middleboxes, for   instance allowing a
middlebox  to remove  itself  from  a session
path. Off-loading further relaxes any dependence on the policy server.

{\bf   Low  performance overhead.}   Recall    that we  identify  each
connection  with a   supersession and  its   associated subsessions. A
simple approach for decomposing a supersession into subsessions can be
implemented by establishing  tunnels between middeboxes.   However, by
adding a  new  header  to   each  packet,  this  approach   introduces  overhead   in the form of MTU  increase    and consequently  packet
fragmentation. Packet fragmentation  can  be solved by  increasing the
MTU of switches  and routers inside  a single  network administration,
but this  solution is not  viable when packets  have  to cross network
domains,  which   has become   common   in virtualized  network
functions that are outsourced to  public clouds.  Moreover,  solutions
that rely on tunnels generally add extra  overheads such as encryption
and compression~\cite{Aplomb}, features  that  can be   redundant or
unnecessary  to   all   flows.  \system   relies   on network  address
translation for explicitly  addressing subsession endpoints, hence the
only overhead is incurred by  port remapping.  Also, supersession  IDs
in \system are not carried on each  packet, as in~\cite{DOA}. They are
stored   on the   middlebox   agents  during  supersession   setup  and reconfiguration.

{\bf  Endpoint  mobility and  NF   migration.}  
%\amy{How  about  renaming    this   sub point    as   ``Separation  of 
%  Functionality'', or   something  that references the   separation of
%  decision-making into 3 planes, or  even something about  simplicity.
%  Separating planes so that  you  can cleanly implement  host mobility
%  and NF migration. In any case, the current title is meaningless.}
The growing trend in network function virtualization can cause dynamic
migration of flows between  middleboxes. Along with frequent  end host
location changes  due to device mobility  or  VM migration,  these new
possibilities  complicate     traffic-steering  solutions  relying  on
administrative control over network routing.  In \system architecture, dynamic policy changes are centrally abstracted by
a  separate policy  server, which decides  (i)   the sequence of  middleboxes
traffic should traverse and  (ii) informs the hosts --- endpoints
or  middleboxes --- of these decisions by pushing  policies ahead of time
to avoid increasing connection set-up  delay.  After fulfilling these two responsibilities, the policy server steps back  to allow  hosts to establish sessions and handle
migrations as needed.

% Incremental deployment
% This will be moved to a different subsection
% 1. Does not rely on a special  naming infrastructure. It can use any
% scheme,  as  long as   the   endpoints can   be  identified  in  the
% subsessions. A different naming scheme, however, requires applications
% and NFs changes.
% 2.  Does not require all the   components to be  integrated into the
% architecture for initial deployment. 
% 3. Does not require application or NF changes.

\subsection{Components}

An overview of  the \system architecture  with its main components and
the    interactions     between   the    modules    is    depicted  in
Figure~\ref{architecture}.   The  architecture     presents    a clear
separation of  management,  control, and  data plane functionalities.
The management  plane   is composed   of  a   policy server that    is
responsible for coordinating the agents on  the control plane based on
high-level network policies provided by the network administrator. The
control  plane implements   the protocol  in  \S\ref{sec:protocol} for
session initiation, flow migration, and network function insertion and
removal.   The data plane is  responsible for translating supersession
IDs  into local IP addresses    and ports, delivering  packets to  the
network functions,  and forwarding   packets between  middleboxes.  To
simplify  the presentation, we first  assume a homogeneous scenario in
which endpoints  and all middleboxes run \system  agents, and defer to
\S\ref{sec:discussion} a discussion of a more realistic scenario where
\system  can be deployed incrementally.  Also,  we assume that network
functions  do  not   change  the  packet  5-tuple, i.e. ---  source and
destination IP addresses  and  port numbers,  and protocol type;  the
case    where     packets  are   changed    will    be   discussed  in
\S\ref{sec:NFsupport}.

\begin{figure}[hb]
\includegraphics[width=\linewidth]{figures/archi.pdf}
\caption{\small\system architecture}\label{architecture}
\end{figure}

% Load   balancing:    Due to the    complex   packet  processing that
% middleboxes run  (e.g.,  deep packet  inspection),  a key  factor in
% middlebox deployments   is to balance the   processing load to avoid
% overload.

% To avoid inference algorithms, which  do not give 100 precision,  we
% may require  NF modifications so they  can interact with the \system
% agents.

% Typical middlebox policies require a packet (or session) to traverse
% a sequence of middlebox (This is  an instance of the broader concept
% of "service chaining").  In our example,  the administrator wants to
% route all HTTP traffic through the policy chain FW-IDS-PROXY and the
% remaining  traffic   through  the   chain FW-IDS.  Note   that  many
% middleboxes  are stateful and need to  process  both directions of a
% session for correctness.

% Middlebox resource management:  Studies show that middlebox overload
% is a  common cause of failures.  

% "The  policy server can take global   decisions to avoid overload or
% give multiple options to the \system agents  so they can balance the
% load across their neighbors.

% SIMPLE 

{\bf Policy  server:}   The  \system  policy  server is   a  logically
centralized server that receives  high-level policy specification from
the  network  administrator and reliably   delivers subpolicies to the
agents, i.e. --- it makes sure that an agent receives a policy consistent
with its location.  For instance,  a client agent should receive  only
policies related to the network it is connected.

The  policy server makes  global decisions based on: (i) configuration
changes  or (ii)   network state changes.    Configuration changes are
triggered  by the network administrator, and  network state changes are
triggered  by monitoring   information, which the  policy  server  receives  from the \system   agents. Based on the monitoring information, the policy server  may  decide to
migrate flows   from an overloaded   network function to   a different
instance, insert new network functions  in a supersession, e.g. --- introduce a DPI after  a light IDS flag packets
for deeper  inspection, or   remove a network
function  out of a  supersession path because it  is no longer needed.
To further extend system flexibility, network administrators can allow the policy server to offload decisions to the \system agents
running on the middleboxes.
A common decision that  can be easily offloaded to  the agents is  the
selection   of a     network   function instance   when  the   network
administrator configures multiple instances of the same type.

%The policy server also decides how a supersession is It determines how
%a supersession is initially divided into subsessions, i.e., it decides
%the middlebox chain a set of flows should traverse in order to enforce
%a network policy.

%- It outsources fine grain  decisions to the  middlebox agents. It can
%send to the middlebox agents a list of MB types to  from the MB chain,
%but the decision to pick an instance is outsourced to  the MBA. It can
%also provide autonomy to a MB to remove itself out the way of a flow.

In  \system,  a
policy  specifies a  predicate,  which defines a set of affected
packets, and a  sequence of network  function types.  This sequence defines the service  chain to be  traversed by the packets  satisfying the predicate, e.g. --- packets from subnet 10.0.1.0/24
should    be     forwarded       through      the     chain     \{Load
Balancer $\rightarrow$ IDS $\rightarrow$ Firewall $\rightarrow$ Proxy\}.    The general form
of a policy is:
\begin{center}
{\tt match(predicate) >> \{NFT$_1 \rightarrow\cdots\rightarrow$ NFT$_n$\}}
\end{center}
%\noindent where
%\begin{center}
%{\tt NFT$_i$ = \{Set of IP addresses\}}
%\end{center}
A predicate is specified with source and  destination IP addresses and
ports, and protocol types.   Complex predicates can be specified using
conjunction (\&),  disjunction  ($|$),  and negation (\~{})  operators
like in Pyretic~\cite{pyretic}. A packet that satisfies  the predicate in the
match  statement is forwarded  through  the chain of network  function
types specified  on the right-hand side of  the  {\tt >>} operator.  A
network   function type ({\tt NFT$_i$})   specifies  a set of  network
function instances  of the same  type. For example the network  administrator can
specify such a set of proxy servers, and
the policy  server will either choose an instance for each client or  delegate   this  decision to  the \system
agents.

%Predicates: The  predicates are specified  with respect to the point a
%packet enters  the  network,  i.e., the  point  of attachment    of an
%endpoint.


{\bf   \system Agents:} \system agents   are key enforcers of
network  policies.   They  must ensure    that  packets are  either correctly
forwarded to the next  middlebox on the  service  chain or dropped  if
they  do not   comply with  the  network policy.   Whenever  a  client
application starts a connection, the \system agent running on the same
machine intercepts the  first packet of  the connection and matches it
with  the   client's network  policies to  determine  the  sequence of
network function  types the packet must traverse.   If a match is
found, the agent  starts the session  setup protocol  in \S\ref{setup}
that   will create the     subsessions  between middleboxes  and   the
corresponding mappings as described below.   The 5-tuple of the client connection
is used to  identify the supersession  packets when they are processed
by  the  network  functions.   Once the  supersession  is   setup, all
subsequent  packets from the client's connection  are forwarded to the
first  middlebox  on  the service  chain,   and  from there  they  are
sequentially forwarded to the remaining middleboxes and to the server.

In each middlebox, a  \system agent must  create two mappings for each
session. One, which we call horizontal NAT,  translates the 5-tuple of
an incoming  packet to the corresponding 5-tuple  that is  used in the
next   subsession.  The   other one,  which   we  call vertical   NAT,
translates   the 5-tuple of each   incoming packet to the supersession
5-tuple before the  packet is delivered to the  network function or to
the applications on  the  endhosts.  Keeping the  supersession 5-tuple
invariant    on  the middleboxes  has  several advantages. First,  it
simplifies policy specification by abstracting the subsessions from the network administrator.  Second,  it simplifies network function state
migration, because network functions  always receive packets  with the
supersession  5-tuple   regardless of flow    migrations or  middlebox
insertions  or removals.  Third,  network functions that do not change
the packet 5-tuple do not need to be changed to work with \system.

\system agents are also heavily involved  in service chain maintenance
and  supersession  reconfigurations.   The  agents report   management
information, such as resource utilization  in the middleboxes, to  the
policy server, so  it  can  make  global management  decisions   about
service chains.  If allowed by network policy, reconfigurations can  also be triggered by a  network
function.   A  common case  of a
reconfiguration triggered by a network function is  when a cache proxy
detects that the content of a session is not cacheable and signals the
\system agent to  remove the  proxy  from the service  chain.  Once  a
reconfiguration is  initiated, either by the policy   server or by the
network function,  \system      agents  execute the  protocols      in
\S\ref{MigrateLogic}.

%{\bf End hosts:}

%\subsection{Division of Labor}

%Clean separation of control, data, and management 

%\subsection{Mechanisms}

